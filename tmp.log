{
  input_h5_text_test : "../../deepbio/dataset/lfma/timelines_test_tfidf.h5"
  weight_decay : 0
  revert_params : -1
  hidden_size : 50000
  optim_epsilon : 1e-08
  val_size : 1000
  model : "simple"
  margin : 0.5
  id : "idcp"
  feat_size_factors : 200
  lookupDropout : 0
  train_size : 198957
  lookup_file : "../../deepbio/dataset/lfma/lookup_tfidf.h5"
  input_h5_factors : "../../deepbio/dataset/lfma/factors_train.h5"
  grad_clip : 0.1
  gpuid : 0
  learning_rate_decay_every : -1
  embedding_dim : 300
  neg_samples : true
  tfidf_file : "../../deepbio/dataset/lfma/tfidf.h5"
  crit : "cosine"
  output_path : "/home/fbarbieri/deepbio/out/"
  seed : 123
  learning_rate_decay : 0.97
  output_size : 200
  beta : 1
  optim : "adam"
  vocab_size : 200000
  dropouts : "{0.1, 0.1}"
  use_pretrained_lu : 1
  batch_size_real : -1
  feat_size_text : 800
  save_output : 0.2
  print_every : 500
  num_layers : 1
  dropout : -1
  test_size : 49988
  batch_size : 30
  max_iters : -1
  checkpoint_path : "cp/"
  save_checkpoint_every : 10000
  learning_rate : 0.0001
  optim_beta : 0.999
  input_h5_text : "../../deepbio/dataset/lfma/timelines_train_tfidf.h5"
  hiddenSizes : "{64, 64}"
  optim_alpha : 0.1
}
Using GPU	
DataLoader loading h5 file (timelines): 	../../deepbio/dataset/lfma/timelines_train_tfidf.h5	
{
  1 : 199957
  2 : 800
}
DataLoader loading h5 file (factors): 	../../deepbio/dataset/lfma/factors_train.h5	
{
  1 : 199957
  2 : 200
}
DataLoader loading h5 file (timelines) -TEST: 	../../deepbio/dataset/lfma/timelines_test_tfidf.h5	
{
  1 : 49988
  2 : 800
}
Assigned to train: 	198957	
Assigned to val: 	1000	
Assigned to test: 	49988	
Loaded lookup: 	
 200000
    300
[torch.LongStorage of size 2]

Loaded tfidf: 	
 200000
      1
[torch.LongStorage of size 2]

Loaded pretrained embeddings	
Loaded pretrained embeddings	
total number of parameters in model: 	60260200	
e:0.08 (i:500) train/val loss: 0.882053/0.858941 sim: 0.141059  bestSim: 0.141059 batch/total time: 0.2397 / 2	
lr= 1.0000e-04 grad norm = 2.3442e+01, param norm = 5.9387e+03, grad/param norm = 3.9473e-03	
e:0.15 (i:1000) train/val loss: 0.863251/0.860055 sim: 0.139945  bestSim: 0.141059 batch/total time: 0.2394 / 4	
lr= 1.0000e-04 grad norm = 2.6669e+01, param norm = 5.9387e+03, grad/param norm = 4.4907e-03	
e:0.23 (i:1500) train/val loss: 0.849304/0.859828 sim: 0.140172  bestSim: 0.141059 batch/total time: 0.2397 / 6	
lr= 1.0000e-04 grad norm = 2.0948e+01, param norm = 5.9387e+03, grad/param norm = 3.5274e-03	
e:0.30 (i:2000) train/val loss: 0.833879/0.857437 sim: 0.142563  bestSim: 0.142563 batch/total time: 0.2395 / 8	
lr= 1.0000e-04 grad norm = 2.4196e+01, param norm = 5.9387e+03, grad/param norm = 4.0742e-03	
e:0.38 (i:2500) train/val loss: 0.868168/0.857339 sim: 0.142661  bestSim: 0.142661 batch/total time: 0.2397 / 10	
lr= 1.0000e-04 grad norm = 2.4896e+01, param norm = 5.9387e+03, grad/param norm = 4.1921e-03	
e:0.45 (i:3000) train/val loss: 0.862737/0.856524 sim: 0.143476  bestSim: 0.143476 batch/total time: 0.2392 / 12	
lr= 1.0000e-04 grad norm = 2.2316e+01, param norm = 5.9388e+03, grad/param norm = 3.7577e-03	
e:0.53 (i:3500) train/val loss: 0.851816/0.857871 sim: 0.142129  bestSim: 0.143476 batch/total time: 0.2677 / 14	
lr= 1.0000e-04 grad norm = 2.5595e+01, param norm = 5.9388e+03, grad/param norm = 4.3098e-03	
e:0.60 (i:4000) train/val loss: 0.838262/0.857040 sim: 0.142960  bestSim: 0.143476 batch/total time: 0.2480 / 16	
lr= 1.0000e-04 grad norm = 2.5163e+01, param norm = 5.9389e+03, grad/param norm = 4.2369e-03	
e:0.68 (i:4500) train/val loss: 0.853519/0.857178 sim: 0.142822  bestSim: 0.143476 batch/total time: 0.2395 / 18	
lr= 1.0000e-04 grad norm = 2.8470e+01, param norm = 5.9390e+03, grad/param norm = 4.7937e-03	
e:0.75 (i:5000) train/val loss: 0.832260/0.855795 sim: 0.144205  bestSim: 0.144205 batch/total time: 0.2395 / 20	
lr= 1.0000e-04 grad norm = 2.2657e+01, param norm = 5.9391e+03, grad/param norm = 3.8149e-03	
